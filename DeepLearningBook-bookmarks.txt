1 1 Introduction
 8 1.1 Who Should Read This Book?
 11 1.2 Historical Trends in Deep Learning
29 I Applied Math and Machine Learning Basics
 31 2 Linear Algebra
  31 2.1 Scalars, Vectors, Matrices and Tensors
  34 2.2 Multiplying Matrices and Vectors
  36 2.3 Identity and Inverse Matrices
  37 2.4 Linear Dependence and Span
  39 2.5 Norms
  40 2.6 Special Kinds of Matrices and Vectors
  42 2.7 Eigendecomposition
  44 2.8 Singular Value Decomposition
  45 2.9 The Moore-Penrose Pseudoinverse
  46 2.10 The Trace Operator
  47 2.11 The Determinant
  48 2.12 Example: Principal Components Analysis
 53 3 Probability and Information Theory
  54 3.1 Why Probability?
  56 3.2 Random Variables
  56 3.3 Probability Distributions
  58 3.4 Marginal Probability
  59 3.5 Conditional Probability
  59 3.6 The Chain Rule of Conditional Probabilities
  60 3.7 Independence and Conditional Independence
  60 3.8 Expectation, Variance and Covariance
  62 3.9 Common Probability Distributions
  67 3.10 Useful Properties of Common Functions
  70 3.11 Bayesâ€™ Rule
  71 3.12 Technical Details of Continuous Variables
  72 3.13 Information Theory
  75 3.14 Structured Probabilistic Models
 80 4 Numerical Computation
  80 4.1 Overflow and Underflow
  82 4.2 Poor Conditioning
  82 4.3 Gradient-Based Optimization
  93 4.4 Constrained Optimization
  96 4.5 Example: Linear Least Squares
 98 5 Machine Learning Basics
  99 5.1 Learning Algorithms
  110 5.2 Capacity, Overfitting and Underfitting
  120 5.3 Hyperparameters and Validation Sets
  122 5.4 Estimators, Bias and Variance
  131 5.5 Maximum Likelihood Estimation
  135 5.6 Bayesian Statistics
  139 5.7 Supervised Learning Algorithms
  145 5.8 Unsupervised Learning Algorithms
  150 5.9 Stochastic Gradient Descent
  152 5.10 Building a Machine Learning Algorithm
  154 5.11 Challenges Motivating Deep Learning
165 II Deep Networks: Modern Practices
 167 6 Deep Feedforward Networks
  170 6.1 Example: Learning XOR
  176 6.2 Gradient-Based Learning
  190 6.3 Hidden Units
  196 6.4 Architecture Design
  203 6.5 Back-Propagation and Other Differentiation Algorithms
  224 6.6 Historical Notes
 228 7 Regularization for Deep Learning
  230 7.1 Parameter Norm Penalties
  237 7.2 Norm Penalties as Constrained Optimization
  239 7.3 Regularization and Under-Constrained Problems
  240 7.4 Dataset Augmentation
  242 7.5 Noise Robustness
  244 7.6 Semi-Supervised Learning
  245 7.7 Multi-Task Learning
  246 7.8 Early Stopping
  251 7.9 Parameter Tying and Parameter Sharing
  253 7.10 Sparse Representations
  255 7.11 Bagging and Other Ensemble Methods
  257 7.12 Dropout
  267 7.13 Adversarial Training
  268 7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier 
 274 8 Optimization for Training Deep Models
  275 8.1 How Learning Differs from Pure Optimization
  282 8.2 Challenges in Neural Network Optimization
  294 8.3 Basic Algorithms
  301 8.4 Parameter Initialization Strategies
  306 8.5 Algorithms with Adaptive Learning Rates
  310 8.6 Approximate Second-Order Methods
  318 8.7 Optimization Strategies and Meta-Algorithms
 331 9 Convolutional Networks
  332 9.1 The Convolution Operation
  336 9.2 Motivation
  340 9.3 Pooling
  346 9.4 Convolution and Pooling as an Infinitely Strong Prior
  348 9.5 Variants of the Basic Convolution Function
  359 9.6 Structured Outputs
  361 9.7 Data Types
  363 9.8 Efficient Convolution Algorithms
  364 9.9 Random or Unsupervised Features
  365 9.10 The Neuroscientific Basis for Convolutional Networks
  372 9.11 Convolutional Networks and the History of Deep Learning
 374 10 Sequence Modeling: Recurrent and Recursive Nets 
  376 10.1 Unfolding Computational Graphs
  379 10.2 Recurrent Neural Networks
  396 10.3 Bidirectional RNNs
  397 10.4 Encoder-Decoder Sequence-to-Sequence Architectures
  399 10.5 Deep Recurrent Networks
  401 10.6 Recursive Neural Networks
  403 10.7 The Challenge of Long-Term Dependencies
  406 10.8 Echo State Networks
  409 10.9 Leaky Units and Other Strategies for Multiple Time Scales
  411 10.10 The Long Short-Term Memory and Other Gated RNNs
  415 10.11 Optimization for Long-Term Dependencies
  419 10.12 Explicit Memory
 424 11 Practical methodology 
  425 11.1 Performance Metrics
  428 11.2 Default Baseline Models
  429 11.3 Determining Whether to Gather More Data
  430 11.4 Selecting Hyperparameters
  439 11.5 Debugging Strategies
  443 11.6 Example: Multi-Digit Number Recognition
 446 12 Applications 
  446 12.1 Large Scale Deep Learning
  455 12.2 Computer Vision
  461 12.3 Speech Recognition
  464 12.4 Natural Language Processing
  480 12.5 Other Applications
489 III Deep Learning Research 
 492 13 Linear Factor Models
  493 13.1 Probabilistic PCA and Factor Analysis
  494 13.2 Independent Component Analysis (ICA)
  496 13.3 Slow Feature Analysis
  499 13.4 Sparse Coding
  502 13.5 Manifold Interpretation of PCA
 505 14 Autoencoders 
  506 14.1 Undercomplete Autoencoders
  507 14.2 Regularized Autoencoders
  511 14.3 Representational Power, Layer Size and Depth
  512 14.4 Stochastic Encoders and Decoders
  513 14.5 Denoising Autoencoders
  518 14.6 Learning Manifolds with Autoencoders
  524 14.7 Contractive Autoencoders
  526 14.8 Predictive Sparse Decomposition
  527 14.9 Applications of Autoencoders
 529 15 Representation Learning 
  531 15.1 Greedy Layer-Wise Unsupervised Pretraining
  539 15.2 Transfer Learning and Domain Adaptation
  544 15.3 Semi-Supervised Disentangling of Causal Factors
  549 15.4 Distributed Representation
  556 15.5 Exponential Gains from Depth
  557 15.6 Providing Clues to Discover Underlying Causes
 561 16 Structured Probabilistic Models for Deep Learning 
  562 16.1 The Challenge of Unstructured Modeling
  566 16.2 Using Graphs to Describe Model Structure
  583 16.3 Sampling from Graphical Models
  584 16.4 Advantages of Structured Modeling
  585 16.5 Learning about Dependencies
  586 16.6 Inference and Approximate Inference
  587 16.7 The Deep Learning Approach to Structured Probabilistic Models 
 593 17 Monte Carlo Methods 
  593 17.1 Sampling and Monte Carlo Methods
  595 17.2 Importance Sampling
  598 17.3 Markov Chain Monte Carlo Methods
  602 17.4 Gibbs Sampling
  602 17.5 The Challenge of Mixing between Separated Modes
 608 18 Confronting the Partition Function 
  609 18.1 The Log-Likelihood Gradient
  610 18.2 Stochastic Maximum Likelihood and Contrastive Divergence
  618 18.3 Pseudolikelihood
  620 18.4 Score Matching and Ratio Matching
  622 18.5 Denoising Score Matching
  623 18.6 Noise-Contrastive Estimation
  626 18.7 Estimating the Partition Function
 634 19 Approximate inference 
  636 19.1 Inference as Optimization
  637 19.2 Expectation Maximization
  638 19.3 MAP Inference and Sparse Coding
  641 19.4 Variational Inference and Learning
  653 19.5 Learned Approximate Inference
 656 20 Deep Generative Models 
  656 20.1 Boltzmann Machines
  658 20.2 Restricted Boltzmann Machines
  662 20.3 Deep Belief Networks
  665 20.4 Deep Boltzmann Machines
  678 20.5 Boltzmann Machines for Real-Valued Data
  685 20.6 Convolutional Boltzmann Machines
  687 20.7 Boltzmann Machines for Structured or Sequential Outputs
  688 20.8 Other Boltzmann Machines
  689 20.9 Back-Propagation through Random Operations
  694 20.10 Directed Generative Nets
  712 20.11 Drawing Samples from Autoencoders
  716 20.12 Generative Stochastic Networks
  717 20.13 Other Generation Schemes
  719 20.14 Evaluating Generative Models
  721 20.15 Conclusion
723 Bibliography
780 Index